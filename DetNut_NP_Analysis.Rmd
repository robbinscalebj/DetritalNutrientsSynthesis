---
title: "DetNut_NP_Analysis"
author: "CJR"
date: "2/5/2022"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(janitor)
library(gratia)
library(mgcv)
library(tidymv)

```

```{r data ingest}


det_raw <- read_csv("C:\\Users\\robbi\\Dropbox\\Detrital Nutrients Synth\\DetNutSynth_Database.csv")

```



```{r create new summary variables, include=FALSE}
det <- det_raw%>%
  group_by(First_Author, Publication_Title, Time_Series_ID)%>%
  mutate(series_index = group_indices())%>%#creates a unique index for each time series
  ungroup()%>%
  group_by(series_index)%>% #functionally, used to confine any window functions like first() to a time series rather than the first observation of whole dataframe
  mutate(initial_C = first(C_per),
         initial_N = first(N_per),
         initial_P = first(P_per),
         initial_CN = first(CN_ratio),
         initial_CP = first(CP_ratio),
         initial_NP = first(NP_ratio),
         initial_Lignin = first(Lignin_per))%>%
  #next normalizes CNP values to initial measurements for each time series and to initial masses as in Manzoni
  mutate(C_prop_initial = C_per/first(C_per),
         N_prop_initial = N_per/first(N_per),
         P_prop_initial = P_per/first(P_per),
         C_mass = ((C_per/100)*Mass_per_remaining),
         N_mass = ((N_per/100)*Mass_per_remaining),
         P_mass = ((P_per/100)*Mass_per_remaining),
         C_mass_norm = ((C_per/100)*Mass_per_remaining)/(first(C_per/100)*first(Mass_per_remaining)),
         N_mass_norm = ((N_per/100)*(Mass_per_remaining/100))/((first(N_per)/100)*first(Mass_per_remaining/100)),
         P_mass_norm = ((P_per/100)*Mass_per_remaining)/(first(P_per/100)*first(Mass_per_remaining)))%>%


  #next approximates percent mass remaining (if not provided) at each measurement day from k value
  mutate(Mass_per_remaining = case_when(is.na(Mass_per_remaining) & Remain_Mass_Category == "afdm" ~ exp(-k*Meas_Day)*100,
                                        is.na(Mass_per_remaining) & Remain_Mass_Category == "dm" ~ exp(-k*Meas_Day)*100,
                                        TRUE ~ Mass_per_remaining))%>%
  #next develops an approximation of N and P availability - imperfect, but aggregates the N and P data together about as well as possible without assuming anything else
  mutate(Water_DIN_ug.L = case_when(is.na(Water_DIN_ug.L) & is.na(Water_NH4_ug.L) ~ Water_NO3_ug.L,
                                    is.na(Water_DIN_ug.L) ~ Water_NO3_ug.L + Water_NH4_ug.L,
                                    TRUE ~ Water_DIN_ug.L),
         TN_approx = case_when(is.na(Water_TN_ug.L) ~ Water_DIN_ug.L,
                               TRUE ~ Water_TN_ug.L),
         TP_approx = case_when(is.na(Water_TP_ug.L) ~ Water_SRP_ug.L,
                               TRUE ~ Water_TP_ug.L))%>%
  #next creates a Temperature, TN, and TP average when there are multiple values given - these NEEDS TO BE CHANGED INTO A TIME-WEIGHTED AVERAGE
  mutate(Temperature_C_avg = case_when(n_distinct(Temperature_C) >1 ~ mean(Temperature_C), 
                                       TRUE ~ Temperature_C),
         TN_approx_avg = case_when(n_distinct(TN_approx) >1 ~ mean(TN_approx), 
                                       TRUE ~ TN_approx),
         TP_approx_avg = case_when(n_distinct(TP_approx) >1 ~ mean(TP_approx), 
                                       TRUE ~ TP_approx))%>%
  mutate(Deg_days = Temperature_C * (Meas_Day-first(Meas_Day)))%>% #degree days
  #remove text from variable if not coarse fine or open

  mutate(mesh_cat = case_when(Mesh_Size_Category == "coarse" | Mesh_Size_Category == "fine" | Mesh_Size_Category == "open" ~ Mesh_Size_Category,
                              Mesh_Size_Category < 1 ~ "fine",
                              Mesh_Size_Category >= 1 ~ "coarse"))%>%
 
  mutate(Lotic_Lentic = case_when(System == "stream"|System == "Stream"|System == "Channel"|System == "river" ~ "Lotic",
                                  System == "wetland"|System == "Wetland"|System == "wetlands"|System == "marsh"|System == "lake"|System == "Lake"|System == "pond"|System == "reservoir" ~ "Lentic",
                                  TRUE ~ System),
         Lotic_Lentic = as_factor(Lotic_Lentic))%>%

  mutate(cum_TN = cumsum(TN_approx_avg)*Meas_Day,
         cum_TP = cumsum(TP_approx_avg)*Meas_Day)%>%
  mutate(series_index = as_factor(series_index),
         Mass_per_loss = 100-(Mass_per_remaining),
         mesh_cat = as_factor(mesh_cat),
         Remain_Mass_Category = as_factor(Remain_Mass_Category))


```

```{r trim data}
det_trim<-det%>%
  filter(Mass_per_remaining<=100)%>%
  filter(Mass_per_remaining >=20)%>%
  filter(!is.na(N_mass_norm))%>%
  filter(N_mass_norm != 0)%>%
  filter(Detritus_Type == "leaves")%>%
  filter(Detritus_Condition == "senesced")%>%
  filter(Remain_Mass_Category == "dm"|Remain_Mass_Category == "afdm")%>%
  filter(mesh_cat != "open")%>%
  mutate(Remain_Mass_Category = fct_drop(Remain_Mass_Category, only = c("dm_postleach", "afdm_postleach", "organic_carbon")))%>%
  mutate(mesh_cat = fct_drop(mesh_cat, "open"))%>%
  ungroup()

levels(det_trim$Remain_Mass_Category)
levels(det_trim$mesh_cat)
levels(det_trim$Lotic_Lentic)

```

We can use generalized additive models (GAMs) to explore global trends of detrital nutrients. This allows us to examine the trends and interacting covariates, as terms with non-linear effects throughout litter breakdown. 

Lotic and Lentic categories have vastly different quantities of measurements. Further, it is another step in complication to include an additional model term to quantify tensor-product interactions in mesh size and habitat (stepping up to very high level interactions on top of base fixed effects). To keep it simple(r), we will qualitatively assess differences in model predictions between habitat/mesh size. 



```{r create separate lotic-lentic fine-coarse datasets}

det_loc <- det_trim%>%filter(mesh_cat == "coarse", Lotic_Lentic == "Lotic")
det_lec <- det_trim%>%filter(mesh_cat == "coarse", Lotic_Lentic == "Lentic")
det_lof <- det_trim%>%filter(mesh_cat == "fine", Lotic_Lentic == "Lotic")
det_lef <- det_trim%>%filter(mesh_cat == "fine", Lotic_Lentic == "Lentic")

```



The overall analysis plan is this: For each response variable (Normalized N or P mass, N or P %, and CNP), we want to test how curves change during litter breakdown - mass loss, and explore how covariates might change those global relationships. 

We always want to control for group-specific variation, which we do with random slope/intercept terms. The random intercept doesn't make sense for response variables where the intercepts are all defined as 1 (and so each series starts at 1) or where one of the covariates defines a time series' intercept (e.g., response N% and covariate initial N %). 

Inference will be based on comparing (AIC, plus residual and k-value checks) two sets of models for each covariate explored. This should work for two reasons: 1) P values are approximate with GAMs, though still useful, and interpreting whether a more complex model is 'better' or 'worse' than a lower level nested model in this case is more robust by also adding in AIC, 2) Keeping the number of terms down in the model is ideal (don't have to worry about including/not including main effects in the correct way). The first set of models for any given covariate will be compared on the same fixed effects structure, but with varying  distributional assumptions. The best model from that tranche will then be compared to a base model (the first that will be defined below), which only models trends of the response variable along Mass loss, controlling for differences due to Remaining Mass Category (Afdm vs DM; preliminary analysis showed these were fairly different and it will be important to account for them). 

There are very few afdm measurements from lentic sites and that creates a substantial problem for analysis in this way. There's simply not enough data even to fit a base model well (at least accounting for afdm/dm differences which can be done differently in a simpler analysis ---), so lentic data need to be analyzed with a simpler, less robust modeling approach, which is fine. But the focus can be on the lotic data.

Thus the common structure of each model type:

Base Model: Y ~ Mass_Loss
Model with covariates:



#Base model
We'll focus for now on the coarse, lotic litter samples, and create a base model that is simply a global trend line accounting for repeated measures and differences in AFDM and DM mass losses

For each of these models, we can choose distributional assumptions via AIC/residuals
```{r Build Base models}

N_base_loc1 <- gam(data = det_loc, N_mass_norm ~  
                        t2(Mass_per_loss,Remain_Mass_Category, bs = c("tp", "re"), full = TRUE) +
                        s(series_index, Mass_per_loss, bs = "re"), method = "REML", family = gaussian(link = "identity"),select = TRUE) 

N_base_loc2 <- gam(data = det_loc, N_mass_norm ~  
                        t2(Mass_per_loss,Remain_Mass_Category, bs = c("tp", "re"), full = TRUE) +
                        s(series_index, Mass_per_loss, bs = "re"), method = "REML", family = gaussian(link = "inverse"),select = TRUE) 

N_base_loc3 <- gam(data = det_loc, N_mass_norm ~  
                        t2(Mass_per_loss,Remain_Mass_Category, bs = c("tp", "re"), full = TRUE) +
                        s(series_index, Mass_per_loss, bs = "re"), method = "REML", family = gaussian(link = "log"),select = TRUE) 

N_base_loc4 <- gam(data = det_loc, N_mass_norm ~  
                        t2(Mass_per_loss,Remain_Mass_Category, bs = c("tp", "re"), full = TRUE) +
                        s(series_index, Mass_per_loss, bs = "re"), method = "REML", family = Gamma(link = "identity"),select = TRUE) 

N_base_loc5 <- gam(data = det_loc, N_mass_norm ~  
                        t2(Mass_per_loss,Remain_Mass_Category, bs = c("tp", "re"), full = TRUE) +
                        s(series_index, Mass_per_loss, bs = "re"), method = "REML", family = Gamma(link = "inverse"),select = TRUE) 

N_base_loc6 <- gam(data = det_loc, N_mass_norm ~  
                        t2(Mass_per_loss,Remain_Mass_Category, bs = c("tp", "re"), full = TRUE) +
                        s(series_index, Mass_per_loss, bs = "re"), method = "REML", family = Gamma(link = "log"),select = TRUE) 




```



```{r Compare and plot base models}
#model comparison and evaluation
base_loc_names<-list(N_base_loc1,N_base_loc2,N_base_loc3,N_base_loc4,N_base_loc5,N_base_loc6)
map(base_loc_names, ~AIC(.))
map(base_loc_names, ~appraise(.))
map(base_loc_names, ~summary(.))

base_loc_mod <- N_base_loc2 #easier name to remember for later comparisons


#plot predictions
base_loc.pred<-predict_gam(type = "link", base_loc_mod, length_out = 5000,  exclude_terms = c("s(series_index)", "s(series_index, Mass_per_loss)"), values = list(series_index = NULL))


 base_loc.p<- ggplot()+
  geom_point(data = det_loc, aes(Mass_per_loss, N_mass_norm), alpha = 0.3)+
    geom_smooth(data = base_loc.pred, se = FALSE, aes(x = Mass_per_loss, y = 1/fit))+
   facet_grid(.~Remain_Mass_Category)+
    geom_abline(slope = -0.01, intercept = 1)+
    #geom_ribbon(data = base_loc.pred, aes(x = Mass_per_loss, ymin = 1/(fit - 2*se.fit), ymax = 1/(fit + 2*se.fit)), alpha = 0.3)+
    NULL
 base_loc.p

 


```
When we're trying to ignore the differences in mass category, we'll sort of ignore this. But note the pretty significant differences here. DM samples really seem to have elevated N mass in later decomp compared to afdm samples. That's an interesting result in itself, but maybe more for a pure mass loss focused study (which we should totally do), but emphasis here is on controlling for that issue to explore N and P time series.

Now we can build up models from our base model. 

#Exploratory analysis
```{r N mass vs initial N}
N_Ni1 <- gam(data = det_loc, N_mass_norm ~  
                        ti(Mass_per_loss, initial_N)+
                        t2(Mass_per_loss,Remain_Mass_Category, bs = c("tp", "re"),full = TRUE) +
                        s(series_index, Mass_per_loss, bs = "re"), method = "REML", family = gaussian(link = "identity"),select = TRUE) 

N_Ni2 <- gam(data = det_loc, N_mass_norm ~
                ti(Mass_per_loss, initial_N)+
                        t2(Mass_per_loss,Remain_Mass_Category, bs = c("tp", "re"),full = TRUE) +
                        s(series_index, Mass_per_loss, bs = "re"), method = "REML", family = gaussian(link = "inverse"),select = TRUE) 

N_Ni3 <- gam(data = det_loc, N_mass_norm ~  
                ti(Mass_per_loss, initial_N)+
                        t2(Mass_per_loss,Remain_Mass_Category, bs = c("tp", "re"),full = TRUE) +
                        s(series_index, Mass_per_loss, bs = "re"), method = "REML", family = gaussian(link = "log"),select = TRUE) 

N_Ni4 <- gam(data = det_loc, N_mass_norm ~  
                ti(Mass_per_loss, initial_N)+
                        t2(Mass_per_loss,Remain_Mass_Category, bs = c("tp", "re"),full = TRUE) +
                        s(series_index, Mass_per_loss, bs = "re"), method = "REML", family = Gamma(link = "identity"),select = TRUE) 

N_Ni5 <- gam(data = det_loc, N_mass_norm ~  
                ti(Mass_per_loss, initial_N)+
                        t2(Mass_per_loss,Remain_Mass_Category, bs = c("tp", "re"),full = TRUE) +
                        s(series_index, Mass_per_loss, bs = "re"), method = "REML", family = Gamma(link = "inverse"),select = TRUE) 

N_Ni6 <- gam(data = det_loc, N_mass_norm ~  
                ti(Mass_per_loss, initial_N)+
                        t2(Mass_per_loss,Remain_Mass_Category, bs = c("tp", "re"),full = TRUE) +
                        s(series_index, Mass_per_loss, bs = "re"), method = "REML", family = Gamma(link = "log"),select = TRUE) 

Ni_mod_names <- list(N_Ni1, N_Ni2,N_Ni3,N_Ni4,N_Ni5,N_Ni6)
map(Ni_mod_names, ~AIC(.)) #2 clear best by AIC
map(Ni_mod_names, ~appraise(.)) #2 less more homoscedastic than others, 5 probably best resids

summary(N_Ni2)
summary(N_Ni5)

AIC(base_loc_mod, N_Ni2, N_Ni5)
```

```{r}
det_c <- det_trim%%>filter(mesh_cat == "coarse")

N.Ni.attempt <- gam(data = det_trim%>%filter(mesh_cat == "coarse"), N_mass_norm ~  
                        te(initial_N,Mass_per_loss, m = 2) + 
                        t2(initial_N,Mass_per_loss,Remain_Mass_Category, bs = c("tp", "tp", "re")) +
                        t2(initial_N,Mass_per_loss, Lotic_Lentic,bs = c("tp", "tp", "re"))  +
                        s(series_index, Mass_per_loss, bs = "re"), method = "REML", family = gaussian(link = "inverse"),select = TRUE) 

gam_pred_ni<-predict_gam(type = "link", N.Ni.attempt, length_out = 1000,  exclude_terms = c("s(series_index)", "s(series_index, Mass_per_loss)"), values = list(series_index = NULL,Remain_Mass_Category = NULL, initial_N = c(0.5,0.8,1.1,1.4,1.7,2.0)))%>%
  mutate(initial_N = as_factor(initial_N))

 N.p<- ggplot()+
  geom_point(data = det_loc, aes(Mass_per_loss, N_mass_norm), alpha = 0.3)+
    geom_smooth(data = gam_pred_ni,se = FALSE, aes(x = Mass_per_loss, y = 1/fit, color = initial_N))+
    #geom_abline(slope = -0.01, intercept = 1)+
    facet_grid(Lotic_Lentic~.)+
    #geom_ribbon(data = gam_pred_ni, aes(x = Mass_per_loss, ymin = 1/(fit - 2*se.fit), ymax = 1/(fit - 2*se.fit)), alpha = 0.3)+
    NULL
 N.p

```

