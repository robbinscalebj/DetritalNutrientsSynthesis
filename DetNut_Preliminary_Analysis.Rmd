---
title: "DetNut Preliminary Analysis"
author: "CJR"
date: "1/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(assertr)
library(janitor)
library(gratia)
library(mgcv)
library(tidymv)

```

```{r data ingest}


det_raw <- read_csv("C:\\Users\\robbi\\Dropbox\\Detrital Nutrients Synth\\DetNutSynth_Database.csv")

```



```{r create new summary variables, include=FALSE}
det <- det_raw%>%
  group_by(First_Author, Publication_Title, Time_Series_ID)%>%
  mutate(series_index = group_indices())%>%#creates a unique index for each time series
  ungroup()%>%
  group_by(series_index)%>% #functionally, used to confine any window functions like first() to a time series rather than the first observation of whole dataframe
  mutate(initial_C = first(C_per),
         initial_N = first(N_per),
         initial_P = first(P_per),
         initial_CN = first(CN_ratio),
         initial_CP = first(CP_ratio),
         initial_NP = first(NP_ratio),
         initial_Lignin = first(Lignin_per))%>%
  #next normalizes CNP values to initial measurements for each time series and to initial masses as in Manzoni
  mutate(C_prop_initial = C_per/first(C_per),
         N_prop_initial = N_per/first(N_per),
         P_prop_initial = P_per/first(P_per),
         C_mass_norm = ((C_per/100)*Mass_per_remaining)/(first(C_per/100)*first(Mass_per_remaining)),
         N_mass_norm = ((N_per/100)*(Mass_per_remaining/100))/((first(N_per)/100)*first(Mass_per_remaining/100)),
         P_mass_norm = ((P_per/100)*Mass_per_remaining)/(first(P_per/100)*first(Mass_per_remaining)))%>%
  #fill in any places where ratios could have been calculated, but weren't

  #next approximates percent mass remaining (if not provided) at each measurement day from k value
  mutate(Mass_per_remaining = case_when(is.na(Mass_per_remaining) & Remain_Mass_Category == "afdm" ~ exp(-k*Meas_Day)*100,
                                        is.na(Mass_per_remaining) & Remain_Mass_Category == "dm" ~ exp(-k*Meas_Day)*100,
                                        TRUE ~ Mass_per_remaining))%>%
  #next develops an approximation of N and P availability - imperfect, but aggregates the N and P data together about as well as possible without assuming anything else
  mutate(Water_DIN_ug.L = case_when(is.na(Water_DIN_ug.L) & is.na(Water_NH4_ug.L) ~ Water_NO3_ug.L,
                                    is.na(Water_DIN_ug.L) ~ Water_NO3_ug.L + Water_NH4_ug.L,
                                    TRUE ~ Water_DIN_ug.L),
         TN_approx = case_when(is.na(Water_TN_ug.L) ~ Water_DIN_ug.L,
                               TRUE ~ Water_TN_ug.L),
         TP_approx = case_when(is.na(Water_TP_ug.L) ~ Water_SRP_ug.L,
                               TRUE ~ Water_TP_ug.L))%>%
  #next creates a Temperature, TN, and TP average when there are multiple values given - these NEEDS TO BE CHANGED INTO A TIME-WEIGHTED AVERAGE
  mutate(Temperature_C_avg = case_when(n_distinct(Temperature_C) >1 ~ mean(Temperature_C), 
                                       TRUE ~ Temperature_C),
         TN_approx_avg = case_when(n_distinct(TN_approx) >1 ~ mean(TN_approx), 
                                       TRUE ~ TN_approx),
         TP_approx_avg = case_when(n_distinct(TP_approx) >1 ~ mean(TP_approx), 
                                       TRUE ~ TP_approx))%>%
  mutate(Deg_days = Temperature_C * (Meas_Day-first(Meas_Day)))%>% #degree days
  #remove text from variable if not coarse fine or open

  mutate(mesh_cat = case_when(Mesh_Size_Category == "coarse" | Mesh_Size_Category == "fine" | Mesh_Size_Category == "open" ~ Mesh_Size_Category,
                              Mesh_Size_Category < 1 ~ "fine",
                              Mesh_Size_Category >= 1 ~ "coarse"))%>%
 
  mutate(Lotic_Lentic = case_when(System == "stream"|System == "Stream"|System == "Channel"|System == "river" ~ "Lotic",
                                  System == "wetland"|System == "Wetland"|System == "wetlands"|System == "marsh"|System == "lake"|System == "Lake"|System == "pond"|System == "reservoir" ~ "Lentic",
                                  TRUE ~ System),
         Lotic_Lentic = as_factor(Lotic_Lentic))%>%

  mutate(cum_TN = cumsum(TN_approx_avg)*Meas_Day,
         cum_TP = cumsum(TP_approx_avg)*Meas_Day)%>%
  mutate(series_index = as_factor(series_index),
         Mass_per_loss = 100-(Mass_per_remaining),
         mesh_cat = as_factor(mesh_cat),
         Remain_Mass_Category = as_factor(Remain_Mass_Category))


```

```{r trim data}
det_trim<-det%>%
  filter(Mass_per_remaining<=100)%>%
  filter(Mass_per_remaining >=20)%>%
  filter(initial_N <3)%>%
  filter(!is.na(N_mass_norm))%>%
  filter(N_mass_norm != 0)%>%
  mutate(weights = ifelse(Mass_per_loss == 0, 1e6, 1))%>%
  filter(Detritus_Type == "leaves")%>%
  filter(Detritus_Condition == "senesced")%>%
  filter(Remain_Mass_Category == "afdm"| Remain_Mass_Category == "dm")%>%
  mutate(mesh_cat = fct_drop(mesh_cat, "open"))%>%
  ungroup()



```

#Modeling Framework
We will use our dataset to establish and explore empirical patterns of detrital nutrient content and stoichiometry, and to base future research needs. We can use GAMs to model non-linear patterns (hypothesized based on terrestrial and some aquatic theory). Modeling will highlight patterns but also limits of our dataset, which we can use to define future research objectives.

We explicitly hypothesize that nutrient-mass loss curves are non-linear, strongly predicted by initial nutrient content, and curve inflections are related to initial nutrient content. Exploratory analysis, when possible, should focus on potential alternative factors that predict (or additionally) predict observed patterns beyond initial nutrient content. Other predictors should be tested with and without initial nutrient content - even if not enough data to look at additional predictors (suggests future research), can rule out other models. Additional predictors include Temperature, external N, external P, velocityv


#N modeling
##Basic N mass loss models
Should collapse these to Coarse and fine models, using 'by' to model the Lotic_Lentic split, and then analyzing fine lotic samples separately. 



```{r Lotic - Coarse}

det_loc <- det_trim%>%
  filter(mesh_cat == "coarse", Lotic_Lentic == "Lotic")%>%
  mutate(N_bin = cut_number(initial_N, n = 5))%>%mutate(N_bin = as_factor(N_bin))
det_loc_Nbins <- levels(det_loc$N_bin)
det_loc<-det_loc%>%
  mutate(N_bin = fct_recode(N_bin, "very low" = det_loc_Nbins[1], low = det_loc_Nbins[2],  med = det_loc_Nbins[3],  high = det_loc_Nbins[4], "very high" = det_loc_Nbins[5]))


N.loc.gam <- gam(data = det_loc, N_mass_norm ~ te(Mass_per_loss, initial_N, bs = "cr") + s(series_index, bs = "re"), method = "REML", family = Gamma(link = "log"))


#gam_pred<-predict_gam(N.loc.gam, exclude_terms = "s(series_index)",  values = list(initial_N = c(0.3,0.4,0.5, 0.6,0.8, 1,1.5,2.0)))%>% mutate(initial_N = as_factor(initial_N))

#how to map N_bin to predicted initial_N values

  ggplot()+
  geom_point(data = det_loc, aes(Mass_per_loss, N_prop_initial, size=initial_N), alpha = 0.3)+
    #geom_point(data = gam_pred, aes(x = Mass_per_loss, y = exp(fit)))+
    #facet_grid(.~N_bin)+
    #scale_x_continuous(limits = c(0,95))+
    #scale_y_continuous(limits = c(0,3), breaks = c(0,0.5,1,2))+
    geom_abline(slope = -1, intercept = 1)+
    NULL
  
  #summary(N.loc.gam)
  #appraise(N.loc.gam)
```
 

```{r Lentic - Coarse}

det_lec <- det_trim%>%
  filter(mesh_cat == "coarse", Lotic_Lentic == "Lentic")
  det_lec_Nbins <- levels(det_loc$N_bin)
det_lec<-det_lec%>%
  mutate(N_bin = fct_recode(N_bin, "very low" = det_lec_Nbins[1], low = det_lec_Nbins[2],  med = det_lec_Nbins[3],  high = det_lec_Nbins[4], "very high" = det_lec_Nbins[5]))

N.lec.gam <- gam(data = det_lec, N_mass_norm ~ te(Mass_per_loss, initial_N,  bs = "bs") + s(series_index, bs = "re"), method = "REML", family = Gamma(link = "log"))


gam_pred<-predict_gam(N.lec.gam, exclude_terms = "s(series_index)",  values = list(initial_N = c(0.3,0.4,0.5, 0.6,0.8, 1,1.5,2.0)))%>%  
  mutate(initial_N = as_factor(initial_N))


  ggplot()+
  #geom_ribbon(data = gam_pred, aes(x = Mass_per_loss, y = exp(fit), ymin = exp(fit-se.fit),ymax = exp(fit+se.fit), color = initial_N))+
  geom_point(data = det_lec, aes(Mass_per_loss, N_mass_norm, color = N_bin), alpha = 0.5)+
    scale_x_continuous(limits = c(0,95))+
    facet_grid(.~N_bin)+
    scale_y_continuous(limits = c(0,3), breaks = c(0,0.5,1,2))+
    geom_abline(slope = -0.01, intercept = 1)+
    NULL
  
   summary(N.lec.gam)
  appraise(N.lec.gam)
```


```{r Lotic - Fine}

det_lof <- det_trim%>%
  filter(mesh_cat == "fine", Lotic_Lentic == "Lotic")%>%mutate(N_bin = cut_number(initial_N, n = 5))%>%mutate(N_bin = as_factor(N_bin))
det_lof_Nbins <- levels(det_loc$N_bin)
det_lof<-det_lof%>%
  mutate(N_bin = fct_recode(N_bin, "very low" = det_lof_Nbins[1], low = det_lof_Nbins[2],  med = det_lof_Nbins[3],  high = det_lof_Nbins[4], "very high" = det_lof_Nbins[5]))

N.lof.gam <- gam(data = det_lof, N_mass_norm ~ te(Mass_per_loss, initial_N,  bs = "cr") , method = "REML", family = Gamma(link = "log"))


gam_pred<-predict_gam(N.lof.gam, exclude_terms = "s(series_index)",  values = list(initial_N = c(0.45,0.7,1.0,2.0,2.5)))%>% mutate(initial_N = as_factor(initial_N))


  ggplot()+
  #geom_point(data = gam_pred, aes(x = Mass_per_loss, y = exp(fit), color = initial_N))+
  geom_point(data = det_lof, aes(1-C_mass_norm, N_mass_norm, color = N_bin), alpha = 0.5)+
    scale_x_continuous(limits = c(0,1))+
    facet_grid(.~N_bin)+
    scale_y_continuous(limits = c(0,3), breaks = c(0,0.5,1,2))+
    geom_abline(slope = -0.01, intercept = 1)+
    NULL
  
    ggplot()+
  geom_point(data = gam_pred, aes(x = Mass_per_loss, y = exp(fit), color = initial_N))+
  geom_point(data = det_lof, aes(Mass_per_loss, N_mass_norm), alpha = 0.5)+
    scale_x_continuous(limits = c(0,95))+
    scale_y_continuous(limits = c(0,3), breaks = c(0,0.5,1,2))+
    geom_abline(slope = -0.01, intercept = 1)+
    NULL
  
  
   summary(N.lof.gam)
  appraise(N.lof.gam)
```

The Lentic-fine model won't fit - not enough data.
```{r Lentic - Fine}

det_lef <- det_trim%>%
  filter(mesh_cat == "fine", Lotic_Lentic == "Lentic")

N.lef.gam <- gam(data = det_lef, N_mass_norm ~ te(Mass_per_loss, initial_N,  bs = "cr") + s(series_index, bs = "re"), method = "REML", family = Gamma(link = "log"))


gam_pred<-predict_gam(N.lef.gam, exclude_terms = "s(series_index)",  values = list(initial_N = c(0.3,0.4,0.5, 0.6,0.8, 1,1.5,2.0,2.5)))%>% mutate(initial_N = as_factor(initial_N))


  ggplot()+
  geom_point(data = gam_pred, aes(x = Mass_per_loss, y = exp(fit), color = initial_N))+
  geom_point(data = det_lef, aes(Mass_per_loss, N_mass_norm), alpha = 0.05)+
    scale_x_continuous(limits = c(0,95))+
    #facet_grid(.~Lotic_Lentic)+
    scale_y_continuous(limits = c(0,3), breaks = c(0,0.5,1,2))+
    geom_abline(slope = -0.01, intercept = 1)+
    NULL
  
```

##Basic N Mass loss~time models
##N Mass exploratory modeling
Test factors: Temp, N, P - models should be with and without initial N content. Even 


#P modeling
##Basic P mass loss models
##Basic P mass loss~time models
##Exploratory P mass modeling


#Stoichiometry - could following be separate paper?
This paper could focus on patterns of stoichiometric convergence a la some of Manning papers
Look at results of everything then decide
#CN modeling
##CN mass loss by initial N
##CN exploratory

#CP modeling
##CP mass loss by initial P
##CP exploratory

#NP modeling
##NP exploratory




